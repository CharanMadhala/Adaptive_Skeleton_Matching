{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36de0d7",
   "metadata": {},
   "source": [
    "## Canonical Skeleton Matching\n",
    "\n",
    "This notebook is organized into three stages:\n",
    "\n",
    "1. Stage 1: Canonical skeleton extraction\n",
    "2. Stage 2: Canonical skeleton visualization (for raw and averaged skeletons, optional)\n",
    "3. Stage 3: Live matching, rep tracking, and voice feedback\n",
    "\n",
    "Files Required to Get Started: exercises_data/bicep_curl/up, exercises_data/bicep_curl/down and main.ipynb\n",
    "\n",
    "Quick start:\n",
    "- Run Stage 1 to generate `up.json` and `down.json` from your dataset.\n",
    "- Use Stage 2 to preview canonical skeletons.\n",
    "- Run the averaging step to produce `up_averaged.json` and `down_averaged.json`.\n",
    "- Run Stage 3 for the live, voice-guided bicep curl tracker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50229f54",
   "metadata": {},
   "source": [
    "### Stage 1: Extraction of canonical skeletons  \n",
    "* It automatically filters out non-visible keypoints based on MediaPipe‚Äôs landmark visibility score.  \n",
    "* It stores only the coordinates of visible joints in each image.  \n",
    "* Each exercise can have a different set of landmarks based on what is visible in its sample images.  \n",
    "  \n",
    "How this helps:  \n",
    "* For half-body exercises, like bicep curls, it might only store shoulders, elbows, wrists, and hips.  \n",
    "* For full-body exercises, like Russian twists, it will include all visible points from head to ankles.  \n",
    "* The system will still use hip midpoint normalization to keep consistency across exercises.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3fc0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.10.14)\n",
      "Requirement already satisfied: numpy in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.0.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mediapipe) (3.10.5)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mediapipe) (4.12.0.88)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jax->mediapipe) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jax->mediapipe) (1.15.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->mediapipe) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->mediapipe) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chara\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python mediapipe numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0929f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèãÔ∏è Processing exercise: bicep_curl\n",
      "  üì∏ Stage: up\n",
      "    ‚úÖ Saved 4 frames to up.json\n",
      "  üì∏ Stage: down\n",
      "    ‚úÖ Saved 5 frames to down.json\n",
      "\n",
      "‚úÖ Canonical skeleton extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Setting up MediaPipe Pose for static image processing\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Config for filtering and normalization\n",
    "# I skip face/head landmarks from Pose\n",
    "FACE_KEYPOINTS = set(range(0, 11))\n",
    "VISIBILITY_THRESHOLD = 0.8  # same threshold\n",
    "\n",
    "# Helper functions I reuse below\n",
    "\n",
    "def get_reference_normalized_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Normalise landmarks based on hip midpoint as origin and\n",
    "    shoulder‚Äìhip distance as scale.\n",
    "    Excludes face/head keypoints and low-visibility landmarks.\n",
    "    \"\"\"\n",
    "    left_hip = np.array([landmarks[23].x, landmarks[23].y, landmarks[23].z])\n",
    "    right_hip = np.array([landmarks[24].x, landmarks[24].y, landmarks[24].z])\n",
    "    origin = (left_hip + right_hip) / 2.0\n",
    "\n",
    "    left_shoulder = np.array([landmarks[11].x, landmarks[11].y, landmarks[11].z])\n",
    "    right_shoulder = np.array([landmarks[12].x, landmarks[12].y, landmarks[12].z])\n",
    "    shoulder_center = (left_shoulder + right_shoulder) / 2.0\n",
    "\n",
    "    scale = np.linalg.norm(shoulder_center - origin)\n",
    "    if scale < 1e-6:\n",
    "        scale = 1.0\n",
    "\n",
    "    normalized = {}\n",
    "    for idx, lm in enumerate(landmarks):\n",
    "        if idx in FACE_KEYPOINTS:\n",
    "            continue\n",
    "        if lm.visibility < VISIBILITY_THRESHOLD:\n",
    "            continue\n",
    "\n",
    "        x, y, z = np.array([lm.x, lm.y, lm.z]) - origin\n",
    "        normalized[str(idx)] = {\n",
    "            'x': float(x / scale),\n",
    "            'y': float(y / scale),\n",
    "            'z': float(z / scale),\n",
    "            'visibility': float(lm.visibility)\n",
    "        }\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def extract_landmarks_from_image(image_path):\n",
    "    \"\"\"Extract normalised landmarks from a single image.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    if not results.pose_landmarks:\n",
    "        return None\n",
    "\n",
    "    return get_reference_normalized_landmarks(results.pose_landmarks.landmark)\n",
    "\n",
    "\n",
    "def save_to_json(file_path, exercise_name, data):\n",
    "    \"\"\"Append new exercise data to the corresponding stage JSON file.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            all_data = json.load(f)\n",
    "    else:\n",
    "        all_data = {}\n",
    "\n",
    "    if exercise_name not in all_data:\n",
    "        all_data[exercise_name] = []\n",
    "\n",
    "    all_data[exercise_name].extend(data)\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(all_data, f, indent=4)\n",
    "\n",
    "\n",
    "# Main extraction pipeline\n",
    "\n",
    "def process_exercises(base_dir='exercises_data'):\n",
    "    \"\"\"\n",
    "    Traverse exercise folders and extract pose landmarks.\n",
    "    Each exercise should contain subfolders: up, down.\n",
    "    \"\"\"\n",
    "    stage_files = {\n",
    "        'up': 'up.json',\n",
    "        'down': 'down.json'\n",
    "    }\n",
    "\n",
    "    for exercise in os.listdir(base_dir):\n",
    "        exercise_path = os.path.join(base_dir, exercise)\n",
    "        if not os.path.isdir(exercise_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüèãÔ∏è Processing exercise: {exercise}\")\n",
    "\n",
    "        for stage, json_file in stage_files.items():\n",
    "            stage_path = os.path.join(exercise_path, stage)\n",
    "            if not os.path.exists(stage_path):\n",
    "                continue\n",
    "\n",
    "            print(f\"  üì∏ Stage: {stage}\")\n",
    "            stage_data = []\n",
    "\n",
    "            for img_name in os.listdir(stage_path):\n",
    "                if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    continue\n",
    "\n",
    "                img_path = os.path.join(stage_path, img_name)\n",
    "                landmarks = extract_landmarks_from_image(img_path)\n",
    "\n",
    "                if landmarks and len(landmarks) > 0:\n",
    "                    stage_data.append(landmarks)\n",
    "\n",
    "            if stage_data:\n",
    "                save_to_json(json_file, exercise, stage_data)\n",
    "                print(f\"    ‚úÖ Saved {len(stage_data)} frames to {json_file}\")\n",
    "            else:\n",
    "                print(f\"    ‚ö†Ô∏è No valid frames found for {stage}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Canonical skeleton extraction complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_exercises()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7531868",
   "metadata": {},
   "source": [
    "### Stage 2: Visualizing canonical skeletons  \n",
    "* Code 1: for up.json and down.json visualization  \n",
    "* Code 2: for up_averaged.json and down_averaged.json visualization  \n",
    "  \n",
    "This script:  \n",
    "* Reads stored JSON files (up.json, down.json)  \n",
    "* Extracts canonical skeletons for each exercise  \n",
    "* Displays them on a black background  \n",
    "* Saves one .png per sample in a folder (canonical_previews_raw)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8f00f",
   "metadata": {},
   "source": [
    "#### Visualizing canonical skeletons (Code 1)\n",
    "\n",
    "Render raw canonical samples from `up.json` and `down.json`. Save previews to `canonical_previews_raw/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6c8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Visualizing stage: up\n",
      "  ‚ûú Exercise: bicep_curl\n",
      "‚úÖ Saved all visualizations for up to 'canonical_previews_raw'\n",
      "\n",
      "üéØ Visualizing stage: down\n",
      "  ‚ûú Exercise: bicep_curl\n",
      "‚úÖ Saved all visualizations for down to 'canonical_previews_raw'\n"
     ]
    }
   ],
   "source": [
    "# Code 1: visualize raw canonical stages\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Setting up MediaPipe Pose drawing\n",
    "mp_pose = mp.solutions.pose\n",
    "POSE_CONNECTIONS = mp_pose.POSE_CONNECTIONS\n",
    "\n",
    "# Inputs and preview output\n",
    "JSON_FILES = [\"up.json\", \"down.json\"]\n",
    "OUTPUT_FOLDER = \"canonical_previews_raw\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "def normalize_and_center(points, image_size=720, padding=0.1):\n",
    "    \"\"\"Scale and center points so the entire skeleton fits nicely in the frame.\"\"\"\n",
    "    points = np.array(points)\n",
    "    min_xy = points.min(axis=0)\n",
    "    max_xy = points.max(axis=0)\n",
    "    center = (max_xy + min_xy) / 2\n",
    "    scale = (1 - 2 * padding) * image_size / max(max_xy - min_xy)\n",
    "\n",
    "    normalized = (points - center) * scale + image_size / 2\n",
    "    return normalized.astype(int)\n",
    "\n",
    "\n",
    "def draw_skeleton(image, landmarks_dict, color_joint=(0, 255, 0), color_line=(255, 255, 128)):\n",
    "    \"\"\"Draw canonical skeleton clearly within image bounds.\"\"\"\n",
    "    coords = []\n",
    "    for idx, lm in landmarks_dict.items():\n",
    "        coords.append([lm[\"x\"], lm[\"y\"]])\n",
    "    coords = np.array(coords)\n",
    "\n",
    "    # Normalize and center points in-frame\n",
    "    coords = normalize_and_center(coords)\n",
    "    idx_list = list(landmarks_dict.keys())\n",
    "    points = {int(idx_list[i]): tuple(coords[i]) for i in range(len(coords))}\n",
    "\n",
    "    # Draw limb connections\n",
    "    for start_idx, end_idx in POSE_CONNECTIONS:\n",
    "        if start_idx in points and end_idx in points:\n",
    "            cv2.line(image, points[start_idx], points[end_idx], color_line, 2)\n",
    "\n",
    "    # Draw joint circles\n",
    "    for pt in points.values():\n",
    "        cv2.circle(image, pt, 5, color_joint, -1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize_json(json_file):\n",
    "    \"\"\"Display all skeletons from one JSON file neatly within the frame.\"\"\"\n",
    "    if not os.path.exists(json_file):\n",
    "        print(f\"‚ö†Ô∏è {json_file} not found, skipping...\")\n",
    "        return\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    stage = os.path.splitext(os.path.basename(json_file))[0]\n",
    "    print(f\"\\nüéØ Visualizing stage: {stage}\")\n",
    "\n",
    "    for exercise_name, skeletons in data.items():\n",
    "        print(f\"  ‚ûú Exercise: {exercise_name}\")\n",
    "        for idx, skeleton in enumerate(skeletons):\n",
    "            img = np.zeros((720, 720, 3), dtype=np.uint8)\n",
    "            img = draw_skeleton(img, skeleton)\n",
    "            cv2.putText(img, f\"{exercise_name.upper()} - {stage.upper()}\",\n",
    "                        (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                        (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            filename = f\"{exercise_name}_{stage}_{idx + 1}.png\"\n",
    "            filepath = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            cv2.imwrite(filepath, img)\n",
    "\n",
    "    print(f\"‚úÖ Saved all visualizations for {stage} to '{OUTPUT_FOLDER}'\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    for json_file in JSON_FILES:\n",
    "        visualize_json(json_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3e92d",
   "metadata": {},
   "source": [
    "#### Additional stage: Averaging canonical skeletons  \n",
    "Store the average keypoints from all collected data in up.json and down.json for each exercise. This helps to have just one image per stage for each exercise.  \n",
    "\n",
    "Script:  \n",
    "* Reads the existing up.json and down.json, which contain multiple samples per exercise, and calculates the average position for each landmark. It then writes up_averaged.json and down_averaged.json.  \n",
    "\n",
    "It computes the mean for each joint across all collected samples and writes:  \n",
    "- up_averaged.json  \n",
    "- down_averaged.json  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98308e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Averaged up ‚Üí up_averaged.json\n",
      "‚úÖ Averaged down ‚Üí down_averaged.json\n"
     ]
    }
   ],
   "source": [
    "import json, os, numpy as np\n",
    "\n",
    "# Stages I average\n",
    "stages = [\"up\", \"down\"]\n",
    "\n",
    "def average_skeletons(stage_file):\n",
    "    \"\"\"Average landmark coordinates across multiple samples per exercise.\"\"\"\n",
    "    with open(stage_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    averaged = {}\n",
    "    for exercise, samples in data.items():\n",
    "        # Grab all landmark keys seen across samples\n",
    "        all_keys = set().union(*[set(s.keys()) for s in samples])\n",
    "        exercise_avg = {}\n",
    "\n",
    "        for key in all_keys:\n",
    "            coords = np.array([[s[key][\"x\"], s[key][\"y\"], s[key][\"z\"]] \n",
    "                               for s in samples if key in s])\n",
    "            if len(coords) > 0:\n",
    "                mean_vals = np.mean(coords, axis=0)\n",
    "                exercise_avg[key] = {\"x\": float(mean_vals[0]),\n",
    "                                     \"y\": float(mean_vals[1]),\n",
    "                                     \"z\": float(mean_vals[2])}\n",
    "        averaged[exercise] = exercise_avg\n",
    "    return averaged\n",
    "\n",
    "\n",
    "for stage in stages:\n",
    "    input_file = f\"{stage}.json\"\n",
    "    output_file = f\"{stage}_averaged.json\"\n",
    "    if os.path.exists(input_file):\n",
    "        avg_data = average_skeletons(input_file)\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(avg_data, f, indent=4)\n",
    "        print(f\"‚úÖ Averaged {stage} ‚Üí {output_file}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Missing {input_file}, skipping...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b48f23",
   "metadata": {},
   "source": [
    "#### Visualizing averaged skeletons (Code 2)\n",
    "\n",
    "Render averaged canonical skeletons from `up_averaged.json` and `down_averaged.json`. Save previews to `canonical_previews_averaged/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44038445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Visualizing stage: up_averaged\n",
      "  ‚ûú Exercise: bicep_curl\n",
      "‚úÖ Saved all visualizations for up_averaged to 'canonical_previews_averaged'\n",
      "\n",
      "üéØ Visualizing stage: down_averaged\n",
      "  ‚ûú Exercise: bicep_curl\n",
      "‚úÖ Saved all visualizations for down_averaged to 'canonical_previews_averaged'\n"
     ]
    }
   ],
   "source": [
    "# Code 2: visualize averaged canonical stages\n",
    "\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Setting up MediaPipe Pose drawing\n",
    "mp_pose = mp.solutions.pose\n",
    "POSE_CONNECTIONS = mp_pose.POSE_CONNECTIONS\n",
    "\n",
    "# Inputs and preview output\n",
    "JSON_FILES = [\"up_averaged.json\", \"down_averaged.json\"]\n",
    "OUTPUT_FOLDER = \"canonical_previews_averaged\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "def normalize_and_center(points, image_size=720, padding=0.1):\n",
    "    \"\"\"Scale and center points so the entire skeleton fits nicely in the frame.\"\"\"\n",
    "    points = np.array(points)\n",
    "    min_xy = points.min(axis=0)\n",
    "    max_xy = points.max(axis=0)\n",
    "    center = (max_xy + min_xy) / 2\n",
    "    scale = (1 - 2 * padding) * image_size / max(max_xy - min_xy)\n",
    "\n",
    "    normalized = (points - center) * scale + image_size / 2\n",
    "    return normalized.astype(int)\n",
    "\n",
    "\n",
    "def draw_skeleton(image, landmarks_dict, color_joint=(0, 255, 0), color_line=(255, 255, 128)):\n",
    "    \"\"\"Draw canonical skeleton clearly within image bounds.\"\"\"\n",
    "    coords = []\n",
    "    for idx, lm in landmarks_dict.items():\n",
    "        coords.append([lm[\"x\"], lm[\"y\"]])\n",
    "    coords = np.array(coords)\n",
    "\n",
    "    # Normalize and center points in-frame\n",
    "    coords = normalize_and_center(coords)\n",
    "    idx_list = list(landmarks_dict.keys())\n",
    "    points = {int(idx_list[i]): tuple(coords[i]) for i in range(len(coords))}\n",
    "\n",
    "    # Draw limb connections\n",
    "    for start_idx, end_idx in POSE_CONNECTIONS:\n",
    "        if start_idx in points and end_idx in points:\n",
    "            cv2.line(image, points[start_idx], points[end_idx], color_line, 2)\n",
    "\n",
    "    # Draw joint circles\n",
    "    for pt in points.values():\n",
    "        cv2.circle(image, pt, 5, color_joint, -1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize_json(json_file):\n",
    "    \"\"\"Display all skeletons from one JSON file neatly within the frame.\"\"\"\n",
    "    if not os.path.exists(json_file):\n",
    "        print(f\"‚ö†Ô∏è {json_file} not found, skipping...\")\n",
    "        return\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    stage = os.path.splitext(os.path.basename(json_file))[0]\n",
    "    print(f\"\\nüéØ Visualizing stage: {stage}\")\n",
    "\n",
    "    for exercise_name, skeletons in data.items():\n",
    "        print(f\"  ‚ûú Exercise: {exercise_name}\")\n",
    "        if isinstance(skeletons, dict):\n",
    "            skeleton_list = [skeletons]  # single averaged skeleton\n",
    "        else:\n",
    "            skeleton_list = skeletons\n",
    "\n",
    "        for idx, skeleton in enumerate(skeleton_list):\n",
    "            img = np.zeros((720, 720, 3), dtype=np.uint8)\n",
    "            img = draw_skeleton(img, skeleton)\n",
    "            cv2.putText(img, f\"{exercise_name.upper()} - {stage.upper()}\",\n",
    "                        (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
    "                        (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            filename = f\"{exercise_name}_{stage}_{idx + 1}.png\"\n",
    "            filepath = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            cv2.imwrite(filepath, img)\n",
    "\n",
    "    print(f\"‚úÖ Saved all visualizations for {stage} to '{OUTPUT_FOLDER}'\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    for json_file in JSON_FILES:\n",
    "        visualize_json(json_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6848bf",
   "metadata": {},
   "source": [
    "### Stage 3: Main logic (Live Matching & Scaling Script):\n",
    "\n",
    "* Opens the webcam and detects pose in real time using MediaPipe.\n",
    "* Loads the averaged skeleton for the selected exercise or stage.\n",
    "* Matches poses based on angles and adjusts scaling for each user. It scales each limb segment‚Äîsuch as shoulder to elbow, elbow to wrist, hip to knee‚Äîso the distances fit the user's proportions.\n",
    "* Automatically moves up, down, and then up again when the user closely matches each stage.\n",
    "* Tracks repetitions and provides voice feedback.\n",
    "* Does not move to the next stage while giving feedback for adjustments, but continues during motivational messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f48739b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∑ Starting camera... Press 'q' to quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# angle_based_bicep_feedback_with_voice_fixed_modified_V2.py\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import threading\n",
    "from math import acos, degrees\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "import tempfile\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ======================\n",
    "# Voice helpers\n",
    "# ======================\n",
    "voice_lock = threading.Lock()\n",
    "is_speaking = False\n",
    "is_feedback_speaking = False # Track when I'm speaking adjustment vs motivation\n",
    "\n",
    "def speak(text, is_adjustment=False):\n",
    "    \"\"\"Speak text asynchronously using gTTS. Sets is_feedback_speaking for adjustments.\"\"\"\n",
    "    global is_speaking, is_feedback_speaking\n",
    "\n",
    "    def _play():\n",
    "        global is_speaking, is_feedback_speaking\n",
    "        with voice_lock:\n",
    "            if is_speaking:\n",
    "                return\n",
    "            is_speaking = True\n",
    "            is_feedback_speaking = is_adjustment # Set flag only for adjustments\n",
    "            \n",
    "        try:\n",
    "            tts = gTTS(text=text, lang=\"en\", tld=\"co.uk\")\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "            tts.save(filename)\n",
    "            playsound.playsound(filename)\n",
    "            os.remove(filename)\n",
    "        except Exception as e:\n",
    "            print(\"Speech error:\", e)\n",
    "        finally:\n",
    "            with voice_lock:\n",
    "                is_speaking = False\n",
    "                is_feedback_speaking = False # Clear flag when finished\n",
    "\n",
    "    threading.Thread(target=_play, daemon=True).start()\n",
    "\n",
    "def trainer_feedback(counter):\n",
    "    motivational = [\n",
    "        \"Great work, keep going!\",\n",
    "        \"Nice form, stay strong!\",\n",
    "        \"Push harder, you got this!\",\n",
    "        \"Excellent! Keep it up!\",\n",
    "        \"Stay focused, don't quit!\"\n",
    "    ]\n",
    "    if counter % 5 == 0:\n",
    "        return f\"{counter}! {random.choice(motivational)}\"\n",
    "    else:\n",
    "        return str(counter)\n",
    "\n",
    "# ===========================\n",
    "# Pose setup and core config\n",
    "# ===========================\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "POSE_CONNECTIONS = mp_pose.POSE_CONNECTIONS\n",
    "pose_detector = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "ANGLE_THRESH_DEG = {\"elbow\": 35.0, \"shoulder\": 35.0, \"wrist\": 35.0, \"torso\": 35.0}\n",
    "REP_MIN_DURATION = 0.5\n",
    "MATCH_HOLD_FRAMES = 6\n",
    "\n",
    "L = {\n",
    "    \"L_SHOULDER\": 11, \"R_SHOULDER\": 12,\n",
    "    \"L_ELBOW\": 13, \"R_ELBOW\": 14,\n",
    "    \"L_WRIST\": 15, \"R_WRIST\": 16,\n",
    "    \"L_HIP\": 23, \"R_HIP\": 24\n",
    "}\n",
    "\n",
    "STAGES = [\"up\", \"down\"]\n",
    "EXERCISE = \"bicep_curl\"\n",
    "\n",
    "# Map joint indices to angle keys (used for green highlights)\n",
    "JOINT_TO_ANGLE_MAP = {\n",
    "    L[\"L_ELBOW\"]: \"L_elbow\", L[\"R_ELBOW\"]: \"R_elbow\",\n",
    "    L[\"L_SHOULDER\"]: \"L_shoulder\", L[\"R_SHOULDER\"]: \"R_shoulder\",\n",
    "    L[\"L_WRIST\"]: \"L_wrist\", L[\"R_WRIST\"]: \"R_wrist\"\n",
    "}\n",
    "\n",
    "# ======================\n",
    "# Geometry helpers\n",
    "# ======================\n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a[:2]), np.array(b[:2]), np.array(c[:2])\n",
    "    ba, bc = a - b, c - b\n",
    "    denom = np.linalg.norm(ba) * np.linalg.norm(bc)\n",
    "    if denom < 1e-8:\n",
    "        return None\n",
    "    cosang = np.clip(np.dot(ba, bc) / denom, -1.0, 1.0)\n",
    "    return degrees(acos(cosang))\n",
    "\n",
    "def torso_angle_deg(user_mid_sh, user_mid_hip):\n",
    "    v = user_mid_sh - user_mid_hip\n",
    "    if np.linalg.norm(v) < 1e-6:\n",
    "        return 0.0\n",
    "    vert = np.array([0.0, -1.0])\n",
    "    cosang = np.clip(np.dot(v, vert) / (np.linalg.norm(v)*np.linalg.norm(vert)), -1.0, 1.0)\n",
    "    return degrees(acos(cosang))\n",
    "\n",
    "def load_averaged(stage_name):\n",
    "    fname = f\"{stage_name}_averaged.json\"\n",
    "    try:\n",
    "        with open(fname, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        return data.get(EXERCISE, {})\n",
    "    except Exception as e:\n",
    "        print(\"Error loading\", fname, e)\n",
    "        return {}\n",
    "\n",
    "def canonical_angles_from_points(canon_points):\n",
    "    angles = {}\n",
    "    def get(i): return np.array([canon_points[str(i)][\"x\"], canon_points[str(i)][\"y\"]]) if str(i) in canon_points else None\n",
    "    L_sh, R_sh = get(L[\"L_SHOULDER\"]), get(L[\"R_SHOULDER\"])\n",
    "    L_el, R_el = get(L[\"L_ELBOW\"]), get(L[\"R_ELBOW\"])\n",
    "    L_wr, R_wr = get(L[\"L_WRIST\"]), get(L[\"R_WRIST\"])\n",
    "    L_hip, R_hip = get(L[\"L_HIP\"]), get(L[\"R_HIP\"])\n",
    "    if L_sh is not None and L_el is not None and L_wr is not None:\n",
    "        angles[\"L_elbow\"] = calculate_angle(L_sh, L_el, L_wr)\n",
    "    if R_sh is not None and R_el is not None and R_wr is not None:\n",
    "        angles[\"R_elbow\"] = calculate_angle(R_sh, R_el, R_wr)\n",
    "    if L_el is not None and L_sh is not None and L_hip is not None:\n",
    "        angles[\"L_shoulder\"] = calculate_angle(L_el, L_sh, L_hip)\n",
    "    if R_el is not None and R_sh is not None and R_hip is not None:\n",
    "        angles[\"R_shoulder\"] = calculate_angle(R_el, R_sh, R_hip)\n",
    "    if L_el is not None and L_wr is not None and L_sh is not None:\n",
    "        angles[\"L_wrist\"] = calculate_angle(L_el, L_wr, L_sh)\n",
    "    if R_el is not None and R_wr is not None and R_sh is not None:\n",
    "        angles[\"R_wrist\"] = calculate_angle(R_el, R_wr, R_sh)\n",
    "    if L_sh is not None and R_sh is not None and L_hip is not None and R_hip is not None:\n",
    "        canon_mid_sh = (L_sh + R_sh)/2\n",
    "        canon_mid_hip = (L_hip + R_hip)/2\n",
    "        v = canon_mid_sh - canon_mid_hip\n",
    "        vert = np.array([0.0, -1.0])\n",
    "        if np.linalg.norm(v) > 1e-8:\n",
    "            cosang = np.clip(np.dot(v, vert)/(np.linalg.norm(v)*np.linalg.norm(vert)),-1.0,1.0)\n",
    "            angles[\"torso\"] = degrees(acos(cosang))\n",
    "    return angles\n",
    "\n",
    "def compute_user_angles(lm):\n",
    "    angles = {}\n",
    "    def Lm(i): return np.array([lm[i].x, lm[i].y]) if i < len(lm) else None\n",
    "    L_sh, R_sh = Lm(L[\"L_SHOULDER\"]), Lm(L[\"R_SHOULDER\"])\n",
    "    L_el, R_el = Lm(L[\"L_ELBOW\"]), Lm(L[\"R_ELBOW\"])\n",
    "    L_wr, R_wr = Lm(L[\"L_WRIST\"]), Lm(L[\"R_WRIST\"])\n",
    "    L_hip, R_hip = Lm(L[\"L_HIP\"]), Lm(L[\"R_HIP\"])\n",
    "    if L_sh is not None and L_el is not None and L_wr is not None:\n",
    "        angles[\"L_elbow\"] = calculate_angle(L_sh, L_el, L_wr)\n",
    "    if R_sh is not None and R_el is not None and R_wr is not None:\n",
    "        angles[\"R_elbow\"] = calculate_angle(R_sh, R_el, R_wr)\n",
    "    if L_el is not None and L_sh is not None and L_hip is not None:\n",
    "        angles[\"L_shoulder\"] = calculate_angle(L_el, L_sh, L_hip)\n",
    "    if R_el is not None and R_sh is not None and R_hip is not None:\n",
    "        angles[\"R_shoulder\"] = calculate_angle(R_el, R_sh, R_hip)\n",
    "    if L_el is not None and L_wr is not None and L_sh is not None:\n",
    "        angles[\"L_wrist\"] = calculate_angle(L_el, L_wr, L_sh)\n",
    "    if R_el is not None and R_wr is not None and R_sh is not None:\n",
    "        angles[\"R_wrist\"] = calculate_angle(R_el, R_wr, R_sh)\n",
    "    if L_sh is not None and R_sh is not None and L_hip is not None and R_hip is not None:\n",
    "        user_mid_sh = (L_sh + R_sh)/2\n",
    "        user_mid_hip = (L_hip + R_hip)/2\n",
    "        angles[\"torso\"] = torso_angle_deg(user_mid_sh, user_mid_hip)\n",
    "    return angles\n",
    "\n",
    "def angle_match_mask(canon_angles_stage, user_angles):\n",
    "    mask, diffs = {}, []\n",
    "    for k, can_ang in canon_angles_stage.items():\n",
    "        ua = user_angles.get(k)\n",
    "        if ua is None: \n",
    "            mask[k] = False\n",
    "            continue\n",
    "        diff = abs(can_ang - ua)\n",
    "        diffs.append(diff)\n",
    "        thr = ANGLE_THRESH_DEG.get(k.split(\"_\")[-1], 25.0)\n",
    "        mask[k] = diff <= thr\n",
    "    return mask, np.mean(diffs) if diffs else 999.0\n",
    "\n",
    "def get_directional_feedback(user_pt, canon_pt, joint_name):\n",
    "    dx, dy = canon_pt[0] - user_pt[0], canon_pt[1] - user_pt[1]\n",
    "    if abs(dx) < 0.01 and abs(dy) < 0.01:\n",
    "        return \"\"\n",
    "    if abs(dy) > abs(dx):\n",
    "        dir_msg = \"lift up\" if dy < 0 else \"lower down\"\n",
    "    else:\n",
    "        dir_msg = \"move inward\" if dx < 0 else \"move outward\"\n",
    "    \n",
    "    # Swap L_/R_ for mirrored camera display\n",
    "    if joint_name.startswith(\"L_\"):\n",
    "        swapped_name = \"R_\" + joint_name[2:] # L_ becomes R_\n",
    "    elif joint_name.startswith(\"R_\"):\n",
    "        swapped_name = \"L_\" + joint_name[2:] # R_ becomes L_\n",
    "    else:\n",
    "        swapped_name = joint_name\n",
    "    # end swap note\n",
    "\n",
    "    joint_name_display = swapped_name.replace(\"L_\", \"Left \").replace(\"R_\", \"Right \")\n",
    "    return f\"Adjust your {joint_name_display.lower().replace('_',' ')}: {dir_msg}\"\n",
    "\n",
    "# =====================\n",
    "# Main loop\n",
    "# =====================\n",
    "canonical_data = {s: load_averaged(s) for s in STAGES}\n",
    "canonical_angles = {s: canonical_angles_from_points(canonical_data[s]) for s in STAGES}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"üì∑ Starting camera... Press 'q' to quit\")\n",
    "\n",
    "rep_count = 0\n",
    "current_stage_idx = 0\n",
    "direction = 1\n",
    "stable_match_frames = 0\n",
    "last_rep_time = None\n",
    "dir_feedback = \"\"\n",
    "spoken_feedback = \"\"\n",
    "display_feedback = \"\"\n",
    "spoken_rep = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose_detector.process(img_rgb)\n",
    "    display = cv2.addWeighted(frame, 0.55, np.zeros_like(frame), 0.45, 0)\n",
    "\n",
    "    stage_name = STAGES[current_stage_idx]\n",
    "    canon_pts = canonical_data.get(stage_name, {})\n",
    "    canon_angles_stage = canonical_angles.get(stage_name, {})\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        lm = results.pose_landmarks.landmark\n",
    "        user_angles = compute_user_angles(lm)\n",
    "        match_mask, mean_diff = angle_match_mask(canon_angles_stage, user_angles)\n",
    "\n",
    "        total_joints = len(match_mask)\n",
    "        matched_joints = sum(1 for m in match_mask.values() if m)\n",
    "        match_ratio = matched_joints / total_joints if total_joints else 0\n",
    "        allow_feedback = match_ratio >= 0.8 # only speak adjustments when most joints are aligned\n",
    "\n",
    "        # Align canonical to the user (scale + rotate)\n",
    "        def align_canonical(canon_pts_dict):\n",
    "            cpts = {int(k): np.array([v[\"x\"], v[\"y\"]]) for k,v in canon_pts_dict.items()}\n",
    "            try:\n",
    "                cLsh, cRsh = cpts[L[\"L_SHOULDER\"]], cpts[L[\"R_SHOULDER\"]]\n",
    "                cLhip, cRhip = cpts[L[\"L_HIP\"]], cpts[L[\"R_HIP\"]]\n",
    "            except: return cpts\n",
    "            canon_mid_sh, canon_mid_hip = (cLsh+cRsh)/2, (cLhip+cRhip)/2\n",
    "            user_L_sh = np.array([lm[L[\"L_SHOULDER\"]].x, lm[L[\"L_SHOULDER\"]].y])\n",
    "            user_R_sh = np.array([lm[L[\"R_SHOULDER\"]].x, lm[L[\"R_SHOULDER\"]].y])\n",
    "            user_L_hip = np.array([lm[L[\"L_HIP\"]].x, lm[L[\"L_HIP\"]].y])\n",
    "            user_R_hip = np.array([lm[L[\"R_HIP\"]].x, lm[L[\"R_HIP\"]].y])\n",
    "            user_mid_sh = (user_L_sh + user_R_sh)/2\n",
    "            user_mid_hip = (user_L_hip + user_R_hip)/2\n",
    "            c_vec, u_vec = canon_mid_sh - canon_mid_hip, user_mid_sh - user_mid_hip\n",
    "            scale = np.linalg.norm(u_vec)/(np.linalg.norm(c_vec)+1e-8)\n",
    "            ang = np.arctan2(u_vec[1],u_vec[0]) - np.arctan2(c_vec[1],c_vec[0])\n",
    "            R = np.array([[np.cos(ang), -np.sin(ang)],[np.sin(ang), np.cos(ang)]])\n",
    "            aligned = {}\n",
    "            for i,p in cpts.items():\n",
    "                shifted=(p-canon_mid_hip)*scale\n",
    "                rotated=R.dot(shifted)\n",
    "                aligned[i]=rotated+user_mid_hip\n",
    "            return aligned\n",
    "\n",
    "        aligned_canon = align_canonical(canon_pts)\n",
    "\n",
    "        # Feedback logic\n",
    "        new_feedback = \"\"\n",
    "        is_adjustment_feedback = False # Flag to mark if this is an adjustment feedback\n",
    "        \n",
    "        if allow_feedback:\n",
    "            for key, idx in {\n",
    "                \"L_elbow\": L[\"L_ELBOW\"], \"R_elbow\": L[\"R_ELBOW\"],\n",
    "                \"L_shoulder\": L[\"L_SHOULDER\"], \"R_shoulder\": L[\"R_SHOULDER\"],\n",
    "                \"L_wrist\": L[\"L_WRIST\"], \"R_wrist\": L[\"R_WRIST\"]\n",
    "            }.items():\n",
    "                # If this angle misses, nudge the user\n",
    "                if key in match_mask and not match_mask[key]:\n",
    "                    if idx in aligned_canon:\n",
    "                        canon_p = aligned_canon[idx]\n",
    "                        user_p = np.array([lm[idx].x, lm[idx].y])\n",
    "                        new_feedback = get_directional_feedback(user_p, canon_p, key)\n",
    "                        is_adjustment_feedback = True\n",
    "                        break\n",
    "\n",
    "        # Only speak the latest feedback for this frame\n",
    "        if allow_feedback and new_feedback and new_feedback != spoken_feedback:\n",
    "            if not is_speaking:\n",
    "                # Pass the is_adjustment flag to the speak function\n",
    "                speak(new_feedback, is_adjustment=is_adjustment_feedback)\n",
    "                spoken_feedback = new_feedback\n",
    "                display_feedback = new_feedback\n",
    "        elif not new_feedback and not is_speaking:\n",
    "            spoken_feedback = \"\"\n",
    "            display_feedback = \"\"\n",
    "\n",
    "        # === Draw skeletons with green matched joints ===\n",
    "        # 1) Draw limb connections\n",
    "        for (s, e) in POSE_CONNECTIONS:\n",
    "            if s in aligned_canon and e in aligned_canon:\n",
    "                p1 = (int(aligned_canon[s][0]*w), int(aligned_canon[s][1]*h))\n",
    "                p2 = (int(aligned_canon[e][0]*w), int(aligned_canon[e][1]*h))\n",
    "                \n",
    "                # light blue/white looks clean over the dark background\n",
    "                color = (200, 200, 255) \n",
    "\n",
    "                cv2.line(display, p1, p2, color, 2)\n",
    "\n",
    "        # 2) Draw key joints; green when the corresponding angle matches\n",
    "        for idx, p in aligned_canon.items():\n",
    "            angle_key = JOINT_TO_ANGLE_MAP.get(idx)\n",
    "            \n",
    "            # Check if the angle associated with this joint is matched\n",
    "            joint_matched = match_mask.get(angle_key, False) if angle_key else False\n",
    "\n",
    "            # green when matched, otherwise red/blue\n",
    "            color = (0, 255, 0) if joint_matched else (0, 0, 255) # Green if matched, Red/Blue otherwise\n",
    "            \n",
    "            cv2.circle(display, (int(p[0]*w), int(p[1]*h)), 6, color, -1)\n",
    "\n",
    "\n",
    "        # Stage switching: only advance when I‚Äôm not speaking an adjustment\n",
    "        all_matched = all(match_mask.values()) if match_mask else False\n",
    "        \n",
    "        if not is_feedback_speaking: # Only proceed if not saying specific adjustment feedback\n",
    "            if all_matched:\n",
    "                stable_match_frames += 1\n",
    "            else:\n",
    "                stable_match_frames = 0\n",
    "\n",
    "            if stable_match_frames >= MATCH_HOLD_FRAMES:\n",
    "                prev_stage = current_stage_idx\n",
    "                current_stage_idx += direction\n",
    "                current_stage_idx = np.clip(current_stage_idx, 0, len(STAGES)-1)\n",
    "                \n",
    "                if current_stage_idx in [0, len(STAGES)-1]:\n",
    "                    direction *= -1\n",
    "                \n",
    "                if STAGES[current_stage_idx] == \"up\":\n",
    "                    now = time.time()\n",
    "                    last_rep_duration = (now - last_rep_time) if last_rep_time else None\n",
    "                    last_rep_time = now\n",
    "                    rep_count += 1\n",
    "                    \n",
    "                    # Give motivational feedback. This is separate and does NOT block stage progression.\n",
    "                    if not new_feedback and not is_speaking:\n",
    "                        speak(trainer_feedback(rep_count))\n",
    "                    spoken_rep = rep_count\n",
    "                \n",
    "                stable_match_frames = 0\n",
    "\n",
    "        # UI overlay\n",
    "        cv2.putText(display, f\"BICEP CURL - {STAGES[current_stage_idx].upper()}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        cv2.putText(display, f\"REPS: {rep_count}\", (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "        # cv2.putText(display, f\"ANG.DIFF: {mean_diff:.1f}¬∞\", (10, 105),\n",
    "        #             cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 2)\n",
    "        if display_feedback:\n",
    "            cv2.putText(display, display_feedback, (10, h - 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Bicep Curl - Directional Feedback\", display)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a4e4f",
   "metadata": {},
   "source": [
    "## Capstone: Gesture-driven workout flow (optional)\n",
    "\n",
    "- Opens the camera and shows a simple menu. \n",
    "- The user selects options by holding a finger pose for about 1.5 seconds. \n",
    "- Uses start and stop hand gestures to control rep tracking during the exercise and to provide a rest or warm-up period. \n",
    "- Returns to the menu after completing the exercise and displays remaining options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "571d3b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\chara\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import threading\n",
    "from math import acos, degrees\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "import tempfile\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# App config\n",
    "# ------------------------------\n",
    "FPS = 30\n",
    "HOLD_FRAMES = int(1 * FPS)   # ~1 second hold\n",
    "MAX_HANDS = 2\n",
    "WINDOW_W, WINDOW_H = 800, 620   # Medium size\n",
    "\n",
    "# Single-exercise flow for now (Bicep Curl)\n",
    "exercises = [\"Bicep Curl\"]\n",
    "\n",
    "# ======================\n",
    "# Voice helpers\n",
    "# ======================\n",
    "voice_lock = threading.Lock()\n",
    "is_speaking = False\n",
    "is_feedback_speaking = False\n",
    "last_adjustment_ts = 0.0\n",
    "ADJUST_COOLDOWN_SEC = 1.2\n",
    "\n",
    "def speak(text, is_adjustment=False):\n",
    "    \"\"\"Speak text asynchronously using gTTS.\"\"\"\n",
    "    global is_speaking, is_feedback_speaking, last_adjustment_ts\n",
    "\n",
    "    def _play():\n",
    "        global is_speaking, is_feedback_speaking\n",
    "        with voice_lock:\n",
    "            if is_speaking:\n",
    "                return\n",
    "            is_speaking = True\n",
    "            is_feedback_speaking = is_adjustment\n",
    "            if is_adjustment:\n",
    "                last_adjustment_ts = time.time()\n",
    "        try:\n",
    "            tts = gTTS(text=text, lang=\"en\", tld=\"co.uk\")\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "            filename = temp_file.name\n",
    "            temp_file.close()\n",
    "            tts.save(filename)\n",
    "            playsound.playsound(filename)\n",
    "            os.remove(filename)\n",
    "        except Exception as e:\n",
    "            print(\"Speech error:\", e)\n",
    "        finally:\n",
    "            with voice_lock:\n",
    "                is_speaking = False\n",
    "                is_feedback_speaking = False\n",
    "\n",
    "    threading.Thread(target=_play, daemon=True).start()\n",
    "\n",
    "def trainer_feedback(counter):\n",
    "    motivational = [\n",
    "        \"Great work, keep going!\",\n",
    "        \"Nice form, stay strong!\",\n",
    "        \"Push harder, you got this!\",\n",
    "        \"Excellent! Keep it up!\",\n",
    "        \"Stay focused, don't quit!\"\n",
    "    ]\n",
    "    if counter % 5 == 0:\n",
    "        return f\"{counter}! {random.choice(motivational)}\"\n",
    "    else:\n",
    "        return str(counter)\n",
    "\n",
    "# ===========================\n",
    "# Pose setup and core config\n",
    "# ===========================\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "POSE_CONNECTIONS = mp_pose.POSE_CONNECTIONS\n",
    "hands_detector = mp_hands.Hands(max_num_hands=MAX_HANDS,\n",
    "                                min_detection_confidence=0.6,\n",
    "                                min_tracking_confidence=0.6)\n",
    "pose_detector = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "ANGLE_THRESH_DEG = {\"elbow\": 35.0, \"shoulder\": 35.0, \"wrist\": 35.0, \"torso\": 35.0}\n",
    "MATCH_HOLD_FRAMES = 6\n",
    "\n",
    "L = {\n",
    "    \"L_SHOULDER\": 11, \"R_SHOULDER\": 12,\n",
    "    \"L_ELBOW\": 13, \"R_ELBOW\": 14,\n",
    "    \"L_WRIST\": 15, \"R_WRIST\": 16,\n",
    "    \"L_HIP\": 23, \"R_HIP\": 24\n",
    "}\n",
    "\n",
    "STAGES = [\"up\", \"down\"]\n",
    "EXERCISE = \"bicep_curl\"\n",
    "\n",
    "JOINT_TO_ANGLE_MAP = {\n",
    "    L[\"L_ELBOW\"]: \"L_elbow\", L[\"R_ELBOW\"]: \"R_elbow\",\n",
    "    L[\"L_SHOULDER\"]: \"L_shoulder\", L[\"R_SHOULDER\"]: \"R_shoulder\",\n",
    "    L[\"L_WRIST\"]: \"L_wrist\", L[\"R_WRIST\"]: \"R_wrist\"\n",
    "}\n",
    "\n",
    "# ======================\n",
    "# Geometry + UI helpers\n",
    "# ======================\n",
    "def calculate_angle(a, b, c):\n",
    "    a, b, c = np.array(a[:2]), np.array(b[:2]), np.array(c[:2])\n",
    "    ba, bc = a - b, c - b\n",
    "    denom = np.linalg.norm(ba) * np.linalg.norm(bc)\n",
    "    if denom < 1e-8:\n",
    "        return None\n",
    "    cosang = np.clip(np.dot(ba, bc) / denom, -1.0, 1.0)\n",
    "    return degrees(acos(cosang))\n",
    "\n",
    "def torso_angle_deg(user_mid_sh, user_mid_hip):\n",
    "    v = user_mid_sh - user_mid_hip\n",
    "    if np.linalg.norm(v) < 1e-6:\n",
    "        return 0.0\n",
    "    vert = np.array([0.0, -1.0])\n",
    "    cosang = np.clip(np.dot(v, vert) / (np.linalg.norm(v)*np.linalg.norm(vert)), -1.0, 1.0)\n",
    "    return degrees(acos(cosang))\n",
    "\n",
    "def load_averaged(stage_name):\n",
    "    fname = f\"{stage_name}_averaged.json\"\n",
    "    try:\n",
    "        with open(fname, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        return data.get(EXERCISE, {})\n",
    "    except Exception as e:\n",
    "        print(\"Error loading\", fname, e)\n",
    "        return {}\n",
    "\n",
    "def canonical_angles_from_points(canon_points):\n",
    "    angles = {}\n",
    "    def get(i): return np.array([canon_points[str(i)][\"x\"], canon_points[str(i)][\"y\"]]) if str(i) in canon_points else None\n",
    "    L_sh, R_sh = get(L[\"L_SHOULDER\"]), get(L[\"R_SHOULDER\"])\n",
    "    L_el, R_el = get(L[\"L_ELBOW\"]), get(L[\"R_ELBOW\"])\n",
    "    L_wr, R_wr = get(L[\"L_WRIST\"]), get(L[\"R_WRIST\"])\n",
    "    L_hip, R_hip = get(L[\"L_HIP\"]), get(L[\"R_HIP\"])\n",
    "    if L_sh is not None and L_el is not None and L_wr is not None:\n",
    "        angles[\"L_elbow\"] = calculate_angle(L_sh, L_el, L_wr)\n",
    "    if R_sh is not None and R_el is not None and R_wr is not None:\n",
    "        angles[\"R_elbow\"] = calculate_angle(R_sh, R_el, R_wr)\n",
    "    if L_el is not None and L_sh is not None and L_hip is not None:\n",
    "        angles[\"L_shoulder\"] = calculate_angle(L_el, L_sh, L_hip)\n",
    "    if R_el is not None and R_sh is not None and R_hip is not None:\n",
    "        angles[\"R_shoulder\"] = calculate_angle(R_el, R_sh, R_hip)\n",
    "    if L_el is not None and L_wr is not None and L_sh is not None:\n",
    "        angles[\"L_wrist\"] = calculate_angle(L_el, L_wr, L_sh)\n",
    "    if R_el is not None and R_wr is not None and R_sh is not None:\n",
    "        angles[\"R_wrist\"] = calculate_angle(R_el, R_wr, R_sh)\n",
    "    if L_sh is not None and R_sh is not None and L_hip is not None and R_hip is not None:\n",
    "        canon_mid_sh = (L_sh + R_sh)/2\n",
    "        canon_mid_hip = (L_hip + R_hip)/2\n",
    "        v = canon_mid_sh - canon_mid_hip\n",
    "        vert = np.array([0.0, -1.0])\n",
    "        if np.linalg.norm(v) > 1e-8:\n",
    "            cosang = np.clip(np.dot(v, vert)/(np.linalg.norm(v)*np.linalg.norm(vert)),-1.0,1.0)\n",
    "            angles[\"torso\"] = degrees(acos(cosang))\n",
    "    return angles\n",
    "\n",
    "def compute_user_angles(lm):\n",
    "    angles = {}\n",
    "    def Lm(i): return np.array([lm[i].x, lm[i].y]) if i < len(lm) else None\n",
    "    L_sh, R_sh = Lm(L[\"L_SHOULDER\"]), Lm(L[\"R_SHOULDER\"])\n",
    "    L_el, R_el = Lm(L[\"L_ELBOW\"]), Lm(L[\"R_ELBOW\"])\n",
    "    L_wr, R_wr = Lm(L[\"L_WRIST\"]), Lm(L[\"R_WRIST\"])\n",
    "    L_hip, R_hip = Lm(L[\"L_HIP\"]), Lm(L[\"R_HIP\"])\n",
    "    if L_sh is not None and L_el is not None and L_wr is not None:\n",
    "        angles[\"L_elbow\"] = calculate_angle(L_sh, L_el, L_wr)\n",
    "    if R_sh is not None and R_el is not None and R_wr is not None:\n",
    "        angles[\"R_elbow\"] = calculate_angle(R_sh, R_el, R_wr)\n",
    "    if L_el is not None and L_sh is not None and L_hip is not None:\n",
    "        angles[\"L_shoulder\"] = calculate_angle(L_el, L_sh, L_hip)\n",
    "    if R_el is not None and R_sh is not None and R_hip is not None:\n",
    "        angles[\"R_shoulder\"] = calculate_angle(R_el, R_sh, R_hip)\n",
    "    if L_el is not None and L_wr is not None and L_sh is not None:\n",
    "        angles[\"L_wrist\"] = calculate_angle(L_el, L_wr, L_sh)\n",
    "    if R_el is not None and R_wr is not None and R_sh is not None:\n",
    "        angles[\"R_wrist\"] = calculate_angle(R_el, R_wr, R_sh)\n",
    "    if L_sh is not None and R_sh is not None and L_hip is not None and R_hip is not None:\n",
    "        user_mid_sh = (L_sh + R_sh)/2\n",
    "        user_mid_hip = (L_hip + R_hip)/2\n",
    "        angles[\"torso\"] = torso_angle_deg(user_mid_sh, user_mid_hip)\n",
    "    return angles\n",
    "\n",
    "def angle_match_mask(canon_angles_stage, user_angles):\n",
    "    mask, diffs = {}, []\n",
    "    for k, can_ang in canon_angles_stage.items():\n",
    "        ua = user_angles.get(k)\n",
    "        if ua is None: \n",
    "            mask[k] = False\n",
    "            continue\n",
    "        diff = abs(can_ang - ua)\n",
    "        diffs.append(diff)\n",
    "        thr = ANGLE_THRESH_DEG.get(k.split(\"_\")[-1], 25.0)\n",
    "        mask[k] = diff <= thr\n",
    "    return mask, np.mean(diffs) if diffs else 999.0\n",
    "\n",
    "def get_directional_feedback(user_pt, canon_pt, joint_name):\n",
    "    dx, dy = canon_pt[0] - user_pt[0], canon_pt[1] - user_pt[1]\n",
    "    if abs(dx) < 0.01 and abs(dy) < 0.01:\n",
    "        return \"\"\n",
    "    if abs(dy) > abs(dx):\n",
    "        dir_msg = \"lift up\" if dy < 0 else \"lower down\"\n",
    "    else:\n",
    "        dir_msg = \"move inward\" if dx < 0 else \"move outward\"\n",
    "    \n",
    "    if joint_name.startswith(\"L_\"):\n",
    "        swapped_name = \"R_\" + joint_name[2:]\n",
    "    elif joint_name.startswith(\"R_\"):\n",
    "        swapped_name = \"L_\" + joint_name[2:]\n",
    "    else:\n",
    "        swapped_name = joint_name\n",
    "\n",
    "    joint_name_display = swapped_name.replace(\"L_\", \"Left \").replace(\"R_\", \"Right \")\n",
    "    return f\"Adjust your {joint_name_display.lower().replace('_',' ')}: {dir_msg}\"\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Text rendering helper\n",
    "# ------------------------------\n",
    "def draw_centered_text(frame, text, y, max_width_frac=0.92, base_scale=1.2, thickness=3, color=(255,255,255)):\n",
    "    h, w = frame.shape[:2]\n",
    "    max_w = int(w * max_width_frac)\n",
    "    scale = base_scale\n",
    "    while scale > 0.3:\n",
    "        (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, scale, thickness)\n",
    "        if tw <= max_w:\n",
    "            x = (w - tw) // 2\n",
    "            cv2.putText(frame, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, scale, color, thickness)\n",
    "            return\n",
    "        scale -= 0.05\n",
    "    (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "    cv2.putText(frame, text, ((w - tw) // 2, y), cv2.FONT_HERSHEY_SIMPLEX, 0.35, color, 1)\n",
    "\n",
    "# ------------------------------\n",
    "# Simple finger counting using relative landmark positions\n",
    "# ------------------------------\n",
    "def count_fingers_single(landmarks, hand_label=\"Right\"):\n",
    "    fingers_open = 0\n",
    "    if hand_label == \"Right\":\n",
    "        if landmarks[4].x < landmarks[3].x:\n",
    "            fingers_open += 1\n",
    "    else:\n",
    "        if landmarks[4].x > landmarks[3].x:\n",
    "            fingers_open += 1\n",
    "\n",
    "    finger_tips = [8, 12, 16, 20]\n",
    "    finger_pips = [6, 10, 14, 18]\n",
    "\n",
    "    for tip, pip in zip(finger_tips, finger_pips):\n",
    "        if landmarks[tip].y < landmarks[pip].y:\n",
    "            fingers_open += 1\n",
    "\n",
    "    return fingers_open\n",
    "\n",
    "def total_finger_count(results):\n",
    "    total = 0\n",
    "    if not results.multi_hand_landmarks or not results.multi_handedness:\n",
    "        return 0\n",
    "    for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "        hand_label = hand_handedness.classification[0].label\n",
    "        total += count_fingers_single(hand_landmarks.landmark, hand_label)\n",
    "    return total\n",
    "\n",
    "def detect_start_single(landmarks, hand_label=\"Right\"):\n",
    "    return count_fingers_single(landmarks, hand_label) >= 4\n",
    "\n",
    "def detect_stop_single(landmarks, hand_label=\"Right\"):\n",
    "    return count_fingers_single(landmarks, hand_label) == 0\n",
    "\n",
    "# ------------------------------\n",
    "# Load averaged canonical data\n",
    "# ------------------------------\n",
    "canonical_data = {s: load_averaged(s) for s in STAGES}\n",
    "canonical_angles = {s: canonical_angles_from_points(canonical_data[s]) for s in STAGES}\n",
    "\n",
    "# ------------------------------\n",
    "# App state\n",
    "# ------------------------------\n",
    "phase = \"menu\"\n",
    "pending_index = None\n",
    "selected_exercise = None\n",
    "exercise_running = False\n",
    "selection_hold = 0\n",
    "start_hold = 0\n",
    "stop_hold = 0\n",
    "\n",
    "# Bicep curl exercise state\n",
    "rep_count = 0\n",
    "current_stage_idx = 0\n",
    "direction = 1\n",
    "stable_match_frames = 0\n",
    "last_rep_time = None\n",
    "spoken_feedback = \"\"\n",
    "display_feedback = \"\"\n",
    "spoken_rep = 0\n",
    "\n",
    "# ------------------------------\n",
    "# Video loop\n",
    "# ------------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Cannot open camera\")\n",
    "\n",
    "cv2.namedWindow(\"Workout Tracker\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Workout Tracker\", WINDOW_W, WINDOW_H)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w, _ = frame.shape\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        hands_results = hands_detector.process(rgb)\n",
    "\n",
    "        # -------- menu --------\n",
    "        if phase == \"menu\":\n",
    "            draw_centered_text(frame,\n",
    "                               \"Hold 1 finger for 1 second to select Bicep Curl\",\n",
    "                               40, base_scale=0.9, thickness=2, color=(0, 255, 0))\n",
    "\n",
    "            x_base, y_base, line_h = 40, 100, 46\n",
    "            for i, ex in enumerate(exercises, start=1):\n",
    "                y = y_base + (i-1) * line_h\n",
    "                color = (0, 255, 0)\n",
    "                if pending_index == i-1:\n",
    "                    cv2.rectangle(frame, (x_base-12, y-30), (x_base+500, y+10), (255, 0, 0), -1)\n",
    "                    color = (255, 255, 255)\n",
    "                cv2.putText(frame, f\"{i}. {ex}\", (x_base, y),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                effective_count = total_finger_count(hands_results)\n",
    "                if effective_count == 1:  # single exercise ‚Üí look for one raised finger\n",
    "                    if pending_index != 0:\n",
    "                        pending_index = 0\n",
    "                        selection_hold = 1\n",
    "                    else:\n",
    "                        selection_hold += 1\n",
    "                else:\n",
    "                    pending_index = None\n",
    "                    selection_hold = 0\n",
    "\n",
    "                if pending_index is not None:\n",
    "                    bar_x1, bar_y1 = 40, h-50\n",
    "                    bar_x2, bar_y2 = w-40, h-20\n",
    "                    progress = min(1.0, selection_hold / HOLD_FRAMES)\n",
    "                    cv2.rectangle(frame, (bar_x1, bar_y1), (bar_x2, bar_y2), (255, 255, 255), 2)\n",
    "                    cv2.rectangle(frame, (bar_x1, bar_y1),\n",
    "                                  (int(bar_x1 + progress*(bar_x2-bar_x1)), bar_y2),\n",
    "                                  (0, 255, 0), -1)\n",
    "\n",
    "                if pending_index is not None and selection_hold >= HOLD_FRAMES:\n",
    "                    selected_exercise = exercises[pending_index]\n",
    "                    phase = \"exercise\"\n",
    "                    exercise_running = False\n",
    "                    pending_index = None\n",
    "                    selection_hold = 0\n",
    "                    # Reset bicep curl state\n",
    "                    rep_count = 0\n",
    "                    current_stage_idx = 0\n",
    "                    direction = 1\n",
    "                    stable_match_frames = 0\n",
    "                    last_rep_time = None\n",
    "                    spoken_feedback = \"\"\n",
    "                    display_feedback = \"\"\n",
    "                    spoken_rep = 0\n",
    "            else:\n",
    "                pending_index = None\n",
    "                selection_hold = 0\n",
    "\n",
    "        # -------- EXERCISE PHASE --------\n",
    "        elif phase == \"exercise\":\n",
    "            # Handle start/stop gestures\n",
    "            if hands_results.multi_hand_landmarks:\n",
    "                first_landmarks = hands_results.multi_hand_landmarks[0].landmark\n",
    "                first_hand_label = hands_results.multi_handedness[0].classification[0].label\n",
    "\n",
    "                if not exercise_running:\n",
    "                    if detect_start_single(first_landmarks, first_hand_label):\n",
    "                        start_hold += 1\n",
    "                    else:\n",
    "                        start_hold = 0\n",
    "                    if start_hold >= HOLD_FRAMES:\n",
    "                        exercise_running = True\n",
    "                        start_hold = 0\n",
    "            else:\n",
    "                start_hold = 0\n",
    "                stop_hold = 0\n",
    "\n",
    "            # If exercise is running, process bicep curl\n",
    "            if exercise_running:\n",
    "                # Check stop gesture without interrupting pose tracking\n",
    "                if hands_results.multi_hand_landmarks:\n",
    "                    first_landmarks = hands_results.multi_hand_landmarks[0].landmark\n",
    "                    first_hand_label = hands_results.multi_handedness[0].classification[0].label\n",
    "                    if detect_stop_single(first_landmarks, first_hand_label):\n",
    "                        stop_hold += 1\n",
    "                    else:\n",
    "                        stop_hold = 0\n",
    "                    if stop_hold >= HOLD_FRAMES:\n",
    "                        exercise_running = False\n",
    "                        stop_hold = 0\n",
    "                        selected_exercise = None\n",
    "                        phase = \"menu\"\n",
    "                        # Reset bicep curl state\n",
    "                        rep_count = 0\n",
    "                        current_stage_idx = 0\n",
    "                        direction = 1\n",
    "                        stable_match_frames = 0\n",
    "                        last_rep_time = None\n",
    "                        spoken_feedback = \"\"\n",
    "                        display_feedback = \"\"\n",
    "                        spoken_rep = 0\n",
    "                        time.sleep(0.2)\n",
    "                        continue\n",
    "\n",
    "                \n",
    "                pose_results = pose_detector.process(rgb)\n",
    "                # display = cv2.addWeighted(frame, 0.55, np.zeros_like(frame), 0.45, 0)\n",
    "                display = cv2.addWeighted(frame, 0.7, np.zeros_like(frame), 0.2, 0)\n",
    "                \n",
    "\n",
    "                stage_name = STAGES[current_stage_idx]\n",
    "                canon_pts = canonical_data.get(stage_name, {})\n",
    "                canon_angles_stage = canonical_angles.get(stage_name, {})\n",
    "\n",
    "                if pose_results.pose_landmarks:\n",
    "                    lm = pose_results.pose_landmarks.landmark\n",
    "                    user_angles = compute_user_angles(lm)\n",
    "                    match_mask, mean_diff = angle_match_mask(canon_angles_stage, user_angles)\n",
    "\n",
    "                    total_joints = len(match_mask)\n",
    "                    matched_joints = sum(1 for m in match_mask.values() if m)\n",
    "                    match_ratio = matched_joints / total_joints if total_joints else 0\n",
    "                    allow_feedback = match_ratio >= 0.8\n",
    "\n",
    "                    # Align canonical skeleton - nested function like in Cell 12\n",
    "                    def align_canonical(canon_pts_dict):\n",
    "                        cpts = {int(k): np.array([v[\"x\"], v[\"y\"]]) for k,v in canon_pts_dict.items()}\n",
    "                        try:\n",
    "                            cLsh, cRsh = cpts[L[\"L_SHOULDER\"]], cpts[L[\"R_SHOULDER\"]]\n",
    "                            cLhip, cRhip = cpts[L[\"L_HIP\"]], cpts[L[\"R_HIP\"]]\n",
    "                        except: return cpts\n",
    "                        canon_mid_sh, canon_mid_hip = (cLsh+cRsh)/2, (cLhip+cRhip)/2\n",
    "                        user_L_sh = np.array([lm[L[\"L_SHOULDER\"]].x, lm[L[\"L_SHOULDER\"]].y])\n",
    "                        user_R_sh = np.array([lm[L[\"R_SHOULDER\"]].x, lm[L[\"R_SHOULDER\"]].y])\n",
    "                        user_L_hip = np.array([lm[L[\"L_HIP\"]].x, lm[L[\"L_HIP\"]].y])\n",
    "                        user_R_hip = np.array([lm[L[\"R_HIP\"]].x, lm[L[\"R_HIP\"]].y])\n",
    "                        user_mid_sh = (user_L_sh + user_R_sh)/2\n",
    "                        user_mid_hip = (user_L_hip + user_R_hip)/2\n",
    "                        c_vec, u_vec = canon_mid_sh - canon_mid_hip, user_mid_sh - user_mid_hip\n",
    "                        scale = np.linalg.norm(u_vec)/(np.linalg.norm(c_vec)+1e-8)\n",
    "                        ang = np.arctan2(u_vec[1],u_vec[0]) - np.arctan2(c_vec[1],c_vec[0])\n",
    "                        R = np.array([[np.cos(ang), -np.sin(ang)],[np.sin(ang), np.cos(ang)]])\n",
    "                        aligned = {}\n",
    "                        for i,p in cpts.items():\n",
    "                            shifted=(p-canon_mid_hip)*scale\n",
    "                            rotated=R.dot(shifted)\n",
    "                            aligned[i]=rotated+user_mid_hip\n",
    "                        return aligned\n",
    "\n",
    "                    aligned_canon = align_canonical(canon_pts)\n",
    "\n",
    "                    # Feedback logic with cooldown to reduce delay and spam\n",
    "                    new_feedback = \"\"\n",
    "                    is_adjustment_feedback = False\n",
    "\n",
    "                    now_ts = time.time()\n",
    "                    can_prompt_adjustment = (now_ts - last_adjustment_ts) >= ADJUST_COOLDOWN_SEC\n",
    "\n",
    "                    if allow_feedback and can_prompt_adjustment:\n",
    "                        for key, idx in {\n",
    "                            \"L_elbow\": L[\"L_ELBOW\"], \"R_elbow\": L[\"R_ELBOW\"],\n",
    "                            \"L_shoulder\": L[\"L_SHOULDER\"], \"R_shoulder\": L[\"R_SHOULDER\"],\n",
    "                            \"L_wrist\": L[\"L_WRIST\"], \"R_wrist\": L[\"R_WRIST\"]\n",
    "                        }.items():\n",
    "                            if key in match_mask and not match_mask[key]:\n",
    "                                if idx in aligned_canon:\n",
    "                                    canon_p = aligned_canon[idx]\n",
    "                                    user_p = np.array([lm[idx].x, lm[idx].y])\n",
    "                                    new_feedback = get_directional_feedback(user_p, canon_p, key)\n",
    "                                    is_adjustment_feedback = True\n",
    "                                    break\n",
    "\n",
    "                    # Speak only current frame's feedback\n",
    "                    if allow_feedback and new_feedback and new_feedback != spoken_feedback:\n",
    "                        if not is_speaking:\n",
    "                            speak(new_feedback, is_adjustment=is_adjustment_feedback)\n",
    "                            spoken_feedback = new_feedback\n",
    "                            display_feedback = new_feedback\n",
    "                    elif not new_feedback and not is_speaking:\n",
    "                        spoken_feedback = \"\"\n",
    "                        display_feedback = \"\"\n",
    "\n",
    "                    # === Draw skeletons with green matched joints ===\n",
    "                    # 1. Draw connections\n",
    "                    for (s, e) in POSE_CONNECTIONS:\n",
    "                        if s in aligned_canon and e in aligned_canon:\n",
    "                            p1 = (int(aligned_canon[s][0]*w), int(aligned_canon[s][1]*h))\n",
    "                            p2 = (int(aligned_canon[e][0]*w), int(aligned_canon[e][1]*h))\n",
    "                            \n",
    "                            # Connection color is light blue/white\n",
    "                            color = (200, 200, 255)\n",
    "\n",
    "                            cv2.line(display, p1, p2, color, 2)\n",
    "\n",
    "                    # 2. Draw key joints (circles) and check for angle match for the green color\n",
    "                    for idx, p in aligned_canon.items():\n",
    "                        angle_key = JOINT_TO_ANGLE_MAP.get(idx)\n",
    "                        \n",
    "                        # Check if the angle associated with this joint is matched\n",
    "                        joint_matched = match_mask.get(angle_key, False) if angle_key else False\n",
    "\n",
    "                        # Set color based on match status\n",
    "                        color = (0, 255, 0) if joint_matched else (0, 0, 255)  # Green if matched, Red/Blue otherwise\n",
    "                        \n",
    "                        cv2.circle(display, (int(p[0]*w), int(p[1]*h)), 6, color, -1)\n",
    "\n",
    "                    # Stage switching logic ‚Äî allow progression when fully matched (even if adjustment speech is ongoing)\n",
    "                    all_matched = all(match_mask.values()) if match_mask else False\n",
    "\n",
    "                    if all_matched:\n",
    "                        stable_match_frames += 1\n",
    "                    else:\n",
    "                        stable_match_frames = 0\n",
    "\n",
    "                    if stable_match_frames >= MATCH_HOLD_FRAMES:\n",
    "                        prev_stage = current_stage_idx\n",
    "                        current_stage_idx += direction\n",
    "                        current_stage_idx = np.clip(current_stage_idx, 0, len(STAGES)-1)\n",
    "                        \n",
    "                        if current_stage_idx in [0, len(STAGES)-1]:\n",
    "                            direction *= -1\n",
    "                        \n",
    "                        if STAGES[current_stage_idx] == \"up\":\n",
    "                            now = time.time()\n",
    "                            last_rep_duration = (now - last_rep_time) if last_rep_time else None\n",
    "                            last_rep_time = now\n",
    "                            rep_count += 1\n",
    "                            \n",
    "                            # Motivational callouts don‚Äôt block stage changes\n",
    "                            if not new_feedback and not is_speaking:\n",
    "                                speak(trainer_feedback(rep_count))\n",
    "                            spoken_rep = rep_count\n",
    "                        \n",
    "                        stable_match_frames = 0\n",
    "\n",
    "                    # UI Info\n",
    "                    cv2.putText(display, f\"BICEP CURL - {STAGES[current_stage_idx].upper()}\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "                    cv2.putText(display, f\"REPS: {rep_count}\", (10, 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                    cv2.putText(display, \"Show CLOSED FIST to STOP\", (10, h - 60),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                    if display_feedback:\n",
    "                        cv2.putText(display, display_feedback, (10, h - 30),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "                    \n",
    "                    frame = display\n",
    "                else:\n",
    "                    # No pose yet ‚Äî keep UI up\n",
    "                    cv2.putText(display, f\"BICEP CURL - {STAGES[current_stage_idx].upper()}\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "                    cv2.putText(display, f\"REPS: {rep_count}\", (10, 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                    cv2.putText(display, \"Waiting for pose detection...\", (10, 110),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                    cv2.putText(display, \"Show CLOSED FIST to STOP\", (10, h - 60),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                    frame = display\n",
    "            else:\n",
    "                # Waiting for start gesture\n",
    "                header = selected_exercise or \"Unknown Exercise\"\n",
    "                draw_centered_text(frame, header, 40, base_scale=1.2, thickness=3, color=(0, 165, 255))\n",
    "                cv2.putText(frame, \"Waiting for START (open palm - 4+ fingers)\", (40, 90),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                \n",
    "                if hands_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        cv2.putText(frame, \"Press 'q' to quit\", (10, h - 12),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "\n",
    "        # display = cv2.addWeighted(frame, 0.55, np.zeros_like(frame), 0.45, 0)\n",
    "        \n",
    "        cv2.imshow(\"Workout Tracker\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    hands_detector.close()\n",
    "    pose_detector.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2bbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
